---
title: "Equations"
author: "Kenny Bartel"
output: 
  html_document:
    toc: true        
    toc_float: true 
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### For sample variance:
$$ s^2 = \frac{1}{n - 1} \sum_{i=1}^{n} (x_i - \bar{x})^2 $$

### For sample standard deviation:
$$ s = \sqrt{\frac{1}{n - 1} \sum_{i=1}^{n} (x_i - \bar{x})^2} $$

### Chebyshev's Theorem Proof

**Statement**: For any real number \( k > 1 \), at least \( 1 - \frac{1}{k^2} \) of the values in a dataset lie within \( k \) standard deviations from the mean.

#### Proof:

1. **Definitions**:
   - Let \( X \) be a random variable with mean \( \mu \) and standard deviation \( \sigma \).
   - Define \( k \) such that \( k > 1 \).

2. **Using Variance**:
   - The variance \( \sigma^2 \) is defined as:
   $$
   \sigma^2 = E[(X - \mu)^2]
   $$

3. **Applying Chebyshevâ€™s Inequality**:
   - We can express the probability that \( X \) deviates from the mean by more than \( k \sigma \):
   $$
   P(|X - \mu| \geq k\sigma) \leq \frac{E[(X - \mu)^2]}{(k\sigma)^2} = \frac{\sigma^2}{k^2\sigma^2} = \frac{1}{k^2}
   $$

4. **Complement**:
   - Therefore, the probability that \( X \) is within \( k \) standard deviations of the mean is:
   $$
   P(|X - \mu| < k\sigma) = 1 - P(|X - \mu| \geq k\sigma) \geq 1 - \frac{1}{k^2}
   $$

5. **Conclusion**:
   - This shows that at least \( 1 - \frac{1}{k^2} \) of the data values lie within \( k \) standard deviations of the mean.

### Chebyshev's inequality:
$$ P\left(|X - \mu| \geq k\sigma\right) \leq \frac{1}{k^2} $$

Which implies:
$$ P\left(|X - \mu| < k\sigma\right) \geq 1 - \frac{1}{k^2} $$


### Sample z-score:
$$ z = \frac{x - \bar{x}}{s} $$

### Population z-score:
$$ z = \frac{x - \mu}{\sigma} $$

### Conditional Probability:
$$ P(A \mid B) = \frac{P(A \cap B)}{P(B)} $$

### Additive rule of mutually exclusive events:
$$ P(A \cup B) = P(A) + P(B) $$

### Multiplicative rule for independent events:
$$ P(A \cap B) = P(A) \times P(B) $$

### Bayes Rule for the Drug Test Problem:
$$
P(\text{User} | \text{Positive}) = \frac{P(positive|user) \cdot P(user)}{P(positive)} = \frac{P(positive|user) \cdot P(user)}{P(positive|user)P(user)+P(positive|nonuser)P(nonuser)} = \frac{0.90 \times 0.05}{(0.90 \times 0.05) + (0.20 \times 0.95)} \approx 0.1915
$$

### Permutations rule given a single set of N distinct elements 
$$ P^N_n = \frac{N!}{(N-n)!} $$

### Example of Permutations Rule:
$P^5_3 = \frac{5!}{(5-3)!} = \frac{5 \times 4 \times 3 \times 2 \times 1}{2 \times 1} = 60$$

### Proof of Permutations Rule:
Theorem 3.2 is known as the Permutation Rule. 
To prove this rule, consider a set of length \( N \). 
Now, you want to determine how many ways you can 
arrange a subset with \( n \) positions, all taken 
from the original set. 

To find this, we randomly select 1 value 
(\( N \)), then another without replacement 
(\( N-1 \)), then another (\( N-2 \)), and so on, 
for a total of \( n \) selections. This results in 
the expression \( N(N-1)(N-2)(N-3)\ldots(N-(n-1)) \).

This simplifies to a ratio of factorials:

\[
\frac{N!}{(N-n)!}
\]

### Partitions Rule:
$$P = \frac{N!}{n_1! n_2! \dots n_k!}$$

### Example of Partitions Rule:
$$\text{Partitions} = \frac{7!}{3!2!2!} = \frac{7 \times 6 \times 5 \times 4 \times 3 \times 2 \times 1}{(3 \times 2 \times 1)(2 \times 1)(2 \times 1)} = \frac{5040}{12} = 420$$

### Proof of Partitions Rule:
Theorem 3.3 is known as the Partitions Rule. 
Assume you have a set of length \( N \) in which 
all values are unique; that is, no values are 
repeated. We aim to demonstrate that 

\[
A = \frac{N!}{n_1! n_2! \ldots n_k!}
\]

This means we want to partition the set into 
\( k \) different subsets. Each subset will 
have lengths of \( n_1, n_2, \ldots, n_k \), 
respectively, such that the sum of these 
lengths equals \( N \):

\[
\sum_{i=1}^{k} n_i = N
\]

By the multiplication rule, each subset will 
have \( n_k(n_k-1)(n_k-2)\ldots1 = n_k! \) 
possible arrangements of elements. 

Using the permutation rule, we can see that 
the unique set can be arranged in \( N \) 
positions as 

\[
N! \cdot (N-N)! = N! \cdot 0! = N!
\]

By applying the multiplication rule once more, 
we arrive at:

\[
N! = A(n_1! n_2! \ldots n_k!)
\]

Thus, we conclude that 

\[
A = \frac{N!}{n_1! n_2! \ldots n_k!}
\]

### Combinations Rule:
$$C^N_n = \binom{N}{n} = \frac{N!}{n!(N-n)!}$$

### Example of Combinations Rule:
$$\text{Example:} \quad C^5_2 = \binom{5}{2} = \frac{5!}{2!(5-2)!} = \frac{5!}{2! \cdot 3!} = \frac{5 \times 4}{2 \times 1} = 10$$

### Proof of Combinations Rule:
Theorem 3.4 is known as the Combinations Rule. 
Assume we have a set of length \( N \) once again. 
We want to determine how many ways a sample of 
\( n \) elements can be chosen from the original set. 

This rule can be viewed as a version of the 
Partitions Rule where \( k = 2 \), with \( n_1 = n! \) 
and \( n_2 = (N-n)! \). Thus, we have the first 
partition consisting of the selected values and 
the second partition consisting of the values 
not selected.

We then see that 

\[
\frac{N!}{n!(N-n)!}
\]

which is denoted as 

\[
\binom{N}{n}
\]

### Multiplicative Rule:
$$
\text{Example:} \quad P(A_1) = 0.5, \quad P(A_2) = 0.3, \quad P(A_3) = 0.2
$$

\text{Then:}
$$
P(A_1 \cap A_2 \cap A_3) = P(A_1) \cdot P(A_2) \cdot P(A_3) = 0.5 \cdot 0.3 \cdot 0.2 = 0.03
$$

### Proof of Multiplicative Rule:
\textbf{Theorem 3.1 (Multiplication Rule)}:

Consider \( k \) sets, where the first set 
contains \( n_1 \) elements, the second set 
contains \( n_2 \) elements, and this 
continues up to the \( k \)-th set with 
\( n_k \) elements. If you aim to create 
combinations by picking one element from 
each set, the total number of possible 
combinations will be the product of the 
sizes of these sets.

Therefore, the total number of combinations 
can be expressed as:

\[
n_1 \times n_2 \times \cdots \times n_k
\]

This is due to the fact that for each of 
the \( n_1 \) selections from the first 
set, there are \( n_2 \) options from the 
second set, and so on, up to \( n_k \) 
options from the \( k \)-th set.

### Pnorm() examples:

#### Find P(Y < 10) where Y ~ N \[(\mu = 10, \sigma = 50)\]

Solution:
```{r}
pnorm(10, mean = 10, sd = 50)
```

#### Find P(Y = 2) where Y ~ N \[(\mu = 10, \sigma = 5)\]

Solution:

This is a continuous normal distribution (Y is a set value), the probability is always 0

#### Find P(Y > 11) where Y ~ N \[(\mu = 10, \sigma = 5)\]

Solution:
```{r}
1 - pnorm(11, mean = 10, sd = 5)
```

#### Find P(3 <= Y < 8) where Y ~ N \[(\mu = 10, \sigma = 5)\]

Solution:
```{r}
pnorm(8, mean = 10, sd = 5) - pnorm(3, mean = 10, sd = 5)
```

#### Find y where Y ~ N \[(\mu = 10, \sigma = 4)\] and P(Y <= y) = 0.6

Solution:
```{r}
qnorm(0.6, mean = 10, sd = 4)
```

#### Find y where Y ~ N \[(\mu = -8, \sigma^2 = 25)\] and P(Y > y) = 0.45

Solution: (first parameter is 0.55 because 1 - 0.45 = 0.55)
```{r}
qnorm(.55, mean = -8, sd = 5)
```

